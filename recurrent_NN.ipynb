{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Autor: Patryk Klytta\n",
        "ZADANIE 1"
      ],
      "metadata": {
        "id": "PJA_z_Dv0-Hu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jJ1K77EL1BDN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50\n",
        "BATCH_SIZE = 16\n",
        "OUTPUT_SEQUENCE_LEN = 7\n",
        "codes = [' ', 'I', 'V', 'X', 'L', 'C', 'D', 'M']"
      ],
      "metadata": {
        "id": "mkHtGGyDKebC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def int_to_roman(num):\n",
        "    val = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n",
        "    syms = ['M', 'CM', 'D', 'CD', 'C', 'XC', 'L', 'XL', 'X', 'IX', 'V', 'IV', 'I']\n",
        "    roman = ''\n",
        "    i = 0\n",
        "    while num > 0:\n",
        "        for _ in range(num // val[i]):\n",
        "            roman += syms[i]\n",
        "            num -= val[i]\n",
        "        i += 1\n",
        "    return roman"
      ],
      "metadata": {
        "id": "I91hgZIa7edV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 1000\n",
        "trainSamples = np.random.randint(1, 201, size=num_samples)\n",
        "trainLabels = [int_to_roman(num) for num in trainSamples]\n",
        "trainSamples_padded = pad_sequences(trainSamples.reshape(-1, 1), maxlen=OUTPUT_SEQUENCE_LEN, padding='post')\n"
      ],
      "metadata": {
        "id": "d1-E927I7lWH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlabels = np.zeros((len(trainLabels), OUTPUT_SEQUENCE_LEN, len(codes)))\n",
        "for i, label in enumerate(trainLabels):\n",
        "    for j in range(OUTPUT_SEQUENCE_LEN):\n",
        "        if j < len(label):\n",
        "            index = codes.index(label[j])\n",
        "            nlabels[i][j][index] = 1\n",
        "        else:\n",
        "            nlabels[i][j][0] = 1"
      ],
      "metadata": {
        "id": "-bWtzOLO7nvO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Embedding(input_dim=201, output_dim=16, input_length=OUTPUT_SEQUENCE_LEN),\n",
        "    LSTM(64, return_sequences=True),\n",
        "    Dense(len(codes), activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yb5KU0A77qhM",
        "outputId": "9d8c3730-07ef-4041-8af4-f5dd28f7e14a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#model.summary()"
      ],
      "metadata": {
        "id": "6jilcw5Y7sC3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(trainSamples_padded, nlabels, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0Dl1C6J784W",
        "outputId": "070be09b-1698-4829-a819-80c9bca1023e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.3469 - loss: 1.8312\n",
            "Epoch 2/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5038 - loss: 1.3086\n",
            "Epoch 3/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5413 - loss: 1.2089\n",
            "Epoch 4/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5949 - loss: 1.1380\n",
            "Epoch 5/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6812 - loss: 0.9186\n",
            "Epoch 6/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7176 - loss: 0.8100\n",
            "Epoch 7/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7418 - loss: 0.7484\n",
            "Epoch 8/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7516 - loss: 0.7173\n",
            "Epoch 9/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7592 - loss: 0.6910\n",
            "Epoch 10/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7682 - loss: 0.6465\n",
            "Epoch 11/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7623 - loss: 0.6501\n",
            "Epoch 12/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7711 - loss: 0.6171\n",
            "Epoch 13/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7769 - loss: 0.5892\n",
            "Epoch 14/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7776 - loss: 0.5667\n",
            "Epoch 15/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7914 - loss: 0.5382\n",
            "Epoch 16/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8158 - loss: 0.5058\n",
            "Epoch 17/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8168 - loss: 0.4829\n",
            "Epoch 18/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8267 - loss: 0.4583\n",
            "Epoch 19/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8375 - loss: 0.4179\n",
            "Epoch 20/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8556 - loss: 0.3838\n",
            "Epoch 21/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8769 - loss: 0.3430\n",
            "Epoch 22/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8890 - loss: 0.3140\n",
            "Epoch 23/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8964 - loss: 0.2831\n",
            "Epoch 24/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9042 - loss: 0.2607\n",
            "Epoch 25/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9112 - loss: 0.2342\n",
            "Epoch 26/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9186 - loss: 0.2130\n",
            "Epoch 27/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9232 - loss: 0.1899\n",
            "Epoch 28/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9260 - loss: 0.1933\n",
            "Epoch 29/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9262 - loss: 0.2107\n",
            "Epoch 30/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9459 - loss: 0.1587\n",
            "Epoch 31/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9521 - loss: 0.1423\n",
            "Epoch 32/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9622 - loss: 0.1222\n",
            "Epoch 33/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9646 - loss: 0.1155\n",
            "Epoch 34/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9713 - loss: 0.1042\n",
            "Epoch 35/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9732 - loss: 0.0961\n",
            "Epoch 36/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9725 - loss: 0.0915\n",
            "Epoch 37/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9799 - loss: 0.0817\n",
            "Epoch 38/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9822 - loss: 0.0716\n",
            "Epoch 39/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9710 - loss: 0.1045\n",
            "Epoch 40/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9689 - loss: 0.1014\n",
            "Epoch 41/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9860 - loss: 0.0703\n",
            "Epoch 42/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9918 - loss: 0.0508\n",
            "Epoch 43/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9954 - loss: 0.0445\n",
            "Epoch 44/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9967 - loss: 0.0402\n",
            "Epoch 45/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9967 - loss: 0.0386\n",
            "Epoch 46/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9988 - loss: 0.0311\n",
            "Epoch 47/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9982 - loss: 0.0312\n",
            "Epoch 48/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9980 - loss: 0.0272\n",
            "Epoch 49/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9989 - loss: 0.0222\n",
            "Epoch 50/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9988 - loss: 0.0222\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7cc71a23a410>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testSamples = np.random.randint(1, 201, size=5).reshape(-1, 1)  # Generowanie testowych liczb\n",
        "testSamples_padded = pad_sequences(testSamples, maxlen=OUTPUT_SEQUENCE_LEN, padding='post')\n",
        "predictions = model.predict(testSamples_padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKcVtXaE7-bB",
        "outputId": "9148552d-4912-4e3f-dbf4-cf85306295b3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, prediction in enumerate(predictions):\n",
        "    decoded = ''\n",
        "    for j in range(OUTPUT_SEQUENCE_LEN):\n",
        "        char_index = np.argmax(prediction[j])  # Największa wartość w wektorze to wybrany znak\n",
        "        decoded += codes[char_index]\n",
        "    print(f'Liczba: {testSamples[i][0]} -> Przewidziane: {decoded.strip()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLCRScYM7_xG",
        "outputId": "01e865ba-1ddb-43ac-ac08-12cf25b6f082"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Liczba: 200 -> Przewidziane: CC\n",
            "Liczba: 168 -> Przewidziane: CLXVIII\n",
            "Liczba: 11 -> Przewidziane: XI\n",
            "Liczba: 124 -> Przewidziane: CXXIV\n",
            "Liczba: 19 -> Przewidziane: XIX\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ZADANIE 2"
      ],
      "metadata": {
        "id": "Y5nv88RJ06Ze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "pVMasYEVLJ8H"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ROMAN_CHARS = ['I', 'V', 'X', 'L', 'C', 'D', 'M']\n",
        "ROMAN_CHAR_INDEX = {char: i + 1 for i, char in enumerate(ROMAN_CHARS)}\n",
        "MAX_LEN = 7\n",
        "\n",
        "def arabic_to_roman(num):\n",
        "    roman_numerals = {\n",
        "        1: \"I\", 4: \"IV\", 5: \"V\", 9: \"IX\", 10: \"X\", 40: \"XL\", 50: \"L\",\n",
        "        90: \"XC\", 100: \"C\", 400: \"CD\", 500: \"D\", 900: \"CM\", 1000: \"M\"\n",
        "    }\n",
        "    result = \"\"\n",
        "    for value, roman in sorted(roman_numerals.items(), key=lambda x: -x[0]):\n",
        "        while num >= value:\n",
        "            result += roman\n",
        "            num -= value\n",
        "    return result\n",
        "\n",
        "def generate_roman_dataset(min_value=1, max_value=200, num_samples=1000):\n",
        "    data = []\n",
        "    for _ in range(num_samples):\n",
        "        value = random.randint(min_value, max_value)\n",
        "        roman = arabic_to_roman(value)\n",
        "        data.append((roman, value))\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "b_iei4W5LaXU"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(dataset):\n",
        "    inputs, outputs = zip(*dataset)\n",
        "    input_sequences = [[ROMAN_CHAR_INDEX[char] for char in seq] for seq in inputs]\n",
        "    padded_inputs = pad_sequences(input_sequences, padding='post')\n",
        "\n",
        "    output_values = np.array(outputs)\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    output_values_scaled = scaler.fit_transform(output_values.reshape(-1, 1))\n",
        "\n",
        "    return padded_inputs, output_values_scaled, scaler"
      ],
      "metadata": {
        "id": "z-MxD1fYMlle"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = generate_roman_dataset()\n",
        "x, y, scaler = prepare_data(dataset)\n",
        "\n",
        "model = Sequential([\n",
        " Embedding(input_dim=len(ROMAN_CHARS) + 1, output_dim=8, input_length=x.shape[1]),\n",
        " LSTM(32, return_sequences=False),\n",
        " Dense(1, activation='linear')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbH7DPMWMoBH",
        "outputId": "001967fc-b844-4029-8394-6bf2ff92e3ec"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "history = model.fit(x, y, epochs=200, batch_size=32, validation_split=0.2,\n",
        "                    callbacks=[early_stop], verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aapl0t_7Mz_9",
        "outputId": "640826fa-246c-4d77-b2a1-221d3b8c8a7c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.2519 - mae: 0.4170 - val_loss: 0.1169 - val_mae: 0.2993\n",
            "Epoch 2/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0859 - mae: 0.2473 - val_loss: 0.0810 - val_mae: 0.2511\n",
            "Epoch 3/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0651 - mae: 0.2219 - val_loss: 0.0404 - val_mae: 0.1695\n",
            "Epoch 4/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0204 - mae: 0.1139 - val_loss: 0.0034 - val_mae: 0.0443\n",
            "Epoch 5/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0027 - mae: 0.0436 - val_loss: 0.0016 - val_mae: 0.0305\n",
            "Epoch 6/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0015 - mae: 0.0318 - val_loss: 0.0012 - val_mae: 0.0256\n",
            "Epoch 7/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0011 - mae: 0.0251 - val_loss: 9.4868e-04 - val_mae: 0.0249\n",
            "Epoch 8/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.9408e-04 - mae: 0.0220 - val_loss: 9.4168e-04 - val_mae: 0.0204\n",
            "Epoch 9/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.0969e-04 - mae: 0.0212 - val_loss: 8.6100e-04 - val_mae: 0.0256\n",
            "Epoch 10/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 6.8902e-04 - mae: 0.0200 - val_loss: 7.8727e-04 - val_mae: 0.0187\n",
            "Epoch 11/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.2248e-04 - mae: 0.0197 - val_loss: 7.1665e-04 - val_mae: 0.0205\n",
            "Epoch 12/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 6.2921e-04 - mae: 0.0184 - val_loss: 7.0263e-04 - val_mae: 0.0178\n",
            "Epoch 13/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.0590e-04 - mae: 0.0180 - val_loss: 6.4231e-04 - val_mae: 0.0201\n",
            "Epoch 14/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.8993e-04 - mae: 0.0177 - val_loss: 6.3815e-04 - val_mae: 0.0172\n",
            "Epoch 15/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.2357e-04 - mae: 0.0167 - val_loss: 5.8283e-04 - val_mae: 0.0193\n",
            "Epoch 16/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.2307e-04 - mae: 0.0146 - val_loss: 5.7444e-04 - val_mae: 0.0203\n",
            "Epoch 17/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.6434e-04 - mae: 0.0164 - val_loss: 5.0331e-04 - val_mae: 0.0171\n",
            "Epoch 18/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.4116e-04 - mae: 0.0155 - val_loss: 5.2954e-04 - val_mae: 0.0196\n",
            "Epoch 19/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.5099e-04 - mae: 0.0163 - val_loss: 4.4572e-04 - val_mae: 0.0156\n",
            "Epoch 20/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.2244e-04 - mae: 0.0151 - val_loss: 4.5454e-04 - val_mae: 0.0165\n",
            "Epoch 21/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.4390e-04 - mae: 0.0135 - val_loss: 4.2816e-04 - val_mae: 0.0171\n",
            "Epoch 22/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.8281e-04 - mae: 0.0148 - val_loss: 3.7655e-04 - val_mae: 0.0148\n",
            "Epoch 23/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.1895e-04 - mae: 0.0133 - val_loss: 5.0452e-04 - val_mae: 0.0201\n",
            "Epoch 24/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.8014e-04 - mae: 0.0148 - val_loss: 3.3317e-04 - val_mae: 0.0141\n",
            "Epoch 25/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.0101e-04 - mae: 0.0129 - val_loss: 3.4735e-04 - val_mae: 0.0125\n",
            "Epoch 26/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.6636e-04 - mae: 0.0118 - val_loss: 3.1480e-04 - val_mae: 0.0120\n",
            "Epoch 27/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.6983e-04 - mae: 0.0122 - val_loss: 2.7557e-04 - val_mae: 0.0126\n",
            "Epoch 28/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.4531e-04 - mae: 0.0118 - val_loss: 2.6803e-04 - val_mae: 0.0115\n",
            "Epoch 29/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.0398e-04 - mae: 0.0106 - val_loss: 2.4259e-04 - val_mae: 0.0110\n",
            "Epoch 30/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.1810e-04 - mae: 0.0106 - val_loss: 2.2126e-04 - val_mae: 0.0109\n",
            "Epoch 31/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0770e-04 - mae: 0.0108 - val_loss: 2.3881e-04 - val_mae: 0.0134\n",
            "Epoch 32/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.1980e-04 - mae: 0.0112 - val_loss: 1.8819e-04 - val_mae: 0.0099\n",
            "Epoch 33/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.0186e-04 - mae: 0.0107 - val_loss: 1.7896e-04 - val_mae: 0.0096\n",
            "Epoch 34/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.8772e-04 - mae: 0.0106 - val_loss: 1.8496e-04 - val_mae: 0.0119\n",
            "Epoch 35/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6069e-04 - mae: 0.0096 - val_loss: 1.6599e-04 - val_mae: 0.0085\n",
            "Epoch 36/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.5629e-04 - mae: 0.0096 - val_loss: 1.7149e-04 - val_mae: 0.0086\n",
            "Epoch 37/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.3099e-04 - mae: 0.0085 - val_loss: 1.6547e-04 - val_mae: 0.0116\n",
            "Epoch 38/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.2549e-04 - mae: 0.0088 - val_loss: 1.1755e-04 - val_mae: 0.0077\n",
            "Epoch 39/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0612e-04 - mae: 0.0080 - val_loss: 1.1569e-04 - val_mae: 0.0094\n",
            "Epoch 40/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.7767e-05 - mae: 0.0078 - val_loss: 1.2124e-04 - val_mae: 0.0075\n",
            "Epoch 41/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.5553e-05 - mae: 0.0075 - val_loss: 8.1787e-05 - val_mae: 0.0066\n",
            "Epoch 42/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.0833e-05 - mae: 0.0064 - val_loss: 7.2104e-05 - val_mae: 0.0066\n",
            "Epoch 43/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.2215e-05 - mae: 0.0067 - val_loss: 7.0824e-05 - val_mae: 0.0060\n",
            "Epoch 44/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.6583e-05 - mae: 0.0062 - val_loss: 8.8345e-05 - val_mae: 0.0076\n",
            "Epoch 45/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.8879e-05 - mae: 0.0065 - val_loss: 5.8987e-05 - val_mae: 0.0057\n",
            "Epoch 46/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.0397e-05 - mae: 0.0062 - val_loss: 5.9350e-05 - val_mae: 0.0057\n",
            "Epoch 47/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.5041e-05 - mae: 0.0052 - val_loss: 4.7815e-05 - val_mae: 0.0055\n",
            "Epoch 48/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.4665e-05 - mae: 0.0052 - val_loss: 4.6444e-05 - val_mae: 0.0053\n",
            "Epoch 49/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.3744e-05 - mae: 0.0052 - val_loss: 4.9724e-05 - val_mae: 0.0050\n",
            "Epoch 50/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.5871e-05 - mae: 0.0047 - val_loss: 3.7743e-05 - val_mae: 0.0049\n",
            "Epoch 51/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.4087e-05 - mae: 0.0052 - val_loss: 3.6310e-05 - val_mae: 0.0046\n",
            "Epoch 52/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.9453e-05 - mae: 0.0050 - val_loss: 3.8366e-05 - val_mae: 0.0050\n",
            "Epoch 53/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.9473e-05 - mae: 0.0050 - val_loss: 5.7355e-05 - val_mae: 0.0057\n",
            "Epoch 54/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.6820e-05 - mae: 0.0054 - val_loss: 3.7690e-05 - val_mae: 0.0045\n",
            "Epoch 55/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.6847e-05 - mae: 0.0048 - val_loss: 2.7386e-05 - val_mae: 0.0041\n",
            "Epoch 56/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.3485e-05 - mae: 0.0045 - val_loss: 2.6473e-05 - val_mae: 0.0041\n",
            "Epoch 57/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.8382e-05 - mae: 0.0043 - val_loss: 3.0367e-05 - val_mae: 0.0045\n",
            "Epoch 58/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.8402e-05 - mae: 0.0042 - val_loss: 2.4056e-05 - val_mae: 0.0039\n",
            "Epoch 59/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5222e-05 - mae: 0.0039 - val_loss: 2.7331e-05 - val_mae: 0.0038\n",
            "Epoch 60/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.4956e-05 - mae: 0.0040 - val_loss: 2.4571e-05 - val_mae: 0.0036\n",
            "Epoch 61/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.1996e-05 - mae: 0.0045 - val_loss: 2.1269e-05 - val_mae: 0.0036\n",
            "Epoch 62/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.4572e-05 - mae: 0.0039 - val_loss: 2.1832e-05 - val_mae: 0.0036\n",
            "Epoch 63/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.1095e-05 - mae: 0.0036 - val_loss: 2.0939e-05 - val_mae: 0.0034\n",
            "Epoch 64/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.8860e-05 - mae: 0.0034 - val_loss: 2.0855e-05 - val_mae: 0.0035\n",
            "Epoch 65/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.7878e-05 - mae: 0.0034 - val_loss: 2.0729e-05 - val_mae: 0.0035\n",
            "Epoch 66/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.3031e-05 - mae: 0.0039 - val_loss: 1.9963e-05 - val_mae: 0.0035\n",
            "Epoch 67/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.1485e-05 - mae: 0.0037 - val_loss: 2.1407e-05 - val_mae: 0.0035\n",
            "Epoch 68/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.8698e-05 - mae: 0.0034 - val_loss: 2.0937e-05 - val_mae: 0.0036\n",
            "Epoch 69/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.8633e-05 - mae: 0.0034 - val_loss: 1.8813e-05 - val_mae: 0.0035\n",
            "Epoch 70/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.9597e-05 - mae: 0.0035 - val_loss: 2.1313e-05 - val_mae: 0.0036\n",
            "Epoch 71/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.1266e-05 - mae: 0.0037 - val_loss: 2.1572e-05 - val_mae: 0.0035\n",
            "Epoch 72/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.8933e-05 - mae: 0.0034 - val_loss: 1.8686e-05 - val_mae: 0.0033\n",
            "Epoch 73/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0341e-05 - mae: 0.0036 - val_loss: 1.7256e-05 - val_mae: 0.0032\n",
            "Epoch 74/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9079e-05 - mae: 0.0036 - val_loss: 1.6400e-05 - val_mae: 0.0032\n",
            "Epoch 75/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7143e-05 - mae: 0.0033 - val_loss: 2.5289e-05 - val_mae: 0.0039\n",
            "Epoch 76/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2350e-05 - mae: 0.0038 - val_loss: 1.8474e-05 - val_mae: 0.0036\n",
            "Epoch 77/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8871e-05 - mae: 0.0035 - val_loss: 1.6815e-05 - val_mae: 0.0034\n",
            "Epoch 78/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.5853e-05 - mae: 0.0032 - val_loss: 1.6321e-05 - val_mae: 0.0033\n",
            "Epoch 79/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7819e-05 - mae: 0.0034 - val_loss: 2.4277e-05 - val_mae: 0.0038\n",
            "Epoch 80/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.7621e-05 - mae: 0.0032 - val_loss: 2.3438e-05 - val_mae: 0.0041\n",
            "Epoch 81/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.1749e-05 - mae: 0.0037 - val_loss: 1.8501e-05 - val_mae: 0.0035\n",
            "Epoch 82/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7926e-05 - mae: 0.0034 - val_loss: 1.5918e-05 - val_mae: 0.0031\n",
            "Epoch 83/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0635e-05 - mae: 0.0037 - val_loss: 1.6029e-05 - val_mae: 0.0031\n",
            "Epoch 84/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.4988e-05 - mae: 0.0030 - val_loss: 4.0323e-05 - val_mae: 0.0054\n",
            "Epoch 85/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.9097e-05 - mae: 0.0044 - val_loss: 1.7645e-05 - val_mae: 0.0033\n",
            "Epoch 86/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6502e-05 - mae: 0.0032 - val_loss: 1.4803e-05 - val_mae: 0.0030\n",
            "Epoch 87/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8772e-05 - mae: 0.0035 - val_loss: 1.4760e-05 - val_mae: 0.0030\n",
            "Epoch 88/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7249e-05 - mae: 0.0034 - val_loss: 1.4576e-05 - val_mae: 0.0030\n",
            "Epoch 89/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.8290e-05 - mae: 0.0035 - val_loss: 2.8396e-05 - val_mae: 0.0044\n",
            "Epoch 90/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.9690e-05 - mae: 0.0045 - val_loss: 1.8289e-05 - val_mae: 0.0035\n",
            "Epoch 91/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9684e-05 - mae: 0.0036 - val_loss: 1.6899e-05 - val_mae: 0.0032\n",
            "Epoch 92/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.5906e-05 - mae: 0.0032 - val_loss: 1.4248e-05 - val_mae: 0.0028\n",
            "Epoch 93/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6167e-05 - mae: 0.0032 - val_loss: 1.3636e-05 - val_mae: 0.0027\n",
            "Epoch 94/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.5925e-05 - mae: 0.0032 - val_loss: 1.6578e-05 - val_mae: 0.0033\n",
            "Epoch 95/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6792e-05 - mae: 0.0033 - val_loss: 2.0327e-05 - val_mae: 0.0037\n",
            "Epoch 96/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2636e-05 - mae: 0.0039 - val_loss: 2.0408e-05 - val_mae: 0.0036\n",
            "Epoch 97/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6307e-05 - mae: 0.0032 - val_loss: 1.8904e-05 - val_mae: 0.0034\n",
            "Epoch 98/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.7996e-05 - mae: 0.0034 - val_loss: 1.7939e-05 - val_mae: 0.0032\n",
            "Epoch 99/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7067e-05 - mae: 0.0033 - val_loss: 1.4339e-05 - val_mae: 0.0031\n",
            "Epoch 100/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.5145e-05 - mae: 0.0031 - val_loss: 1.9081e-05 - val_mae: 0.0033\n",
            "Epoch 101/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9742e-05 - mae: 0.0036 - val_loss: 2.0907e-05 - val_mae: 0.0036\n",
            "Epoch 102/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.5511e-05 - mae: 0.0032 - val_loss: 1.4452e-05 - val_mae: 0.0029\n",
            "Epoch 103/200\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7555e-05 - mae: 0.0033 - val_loss: 2.0515e-05 - val_mae: 0.0035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_roman_to_arabic(roman_str, scaler):\n",
        "    input_seq = [[ROMAN_CHAR_INDEX[char] for char in roman_str]]\n",
        "    padded_input = pad_sequences(input_seq, padding='post', maxlen=x.shape[1])\n",
        "    prediction_scaled = model.predict(padded_input, verbose=0)\n",
        "\n",
        "    prediction = scaler.inverse_transform(prediction_scaled)\n",
        "    return round(prediction[0][0])\n",
        "\n",
        "def evaluate_model(test_data, scaler):\n",
        "    results = []\n",
        "    for roman, actual in test_data:\n",
        "        predicted = predict_roman_to_arabic(roman, scaler)\n",
        "        results.append({\n",
        "            'Roman': roman,\n",
        "            'Actual Arabic': actual,\n",
        "            'Predicted Arabic': predicted,\n",
        "            'Error': abs(predicted - actual)\n",
        "        })\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "test_dataset = generate_roman_dataset(num_samples=200)\n",
        "evaluation_results = evaluate_model(test_dataset, scaler)\n",
        "\n",
        "print(\"\\nWyniki:\")\n",
        "print(evaluation_results.head(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZAXhyPo2_62",
        "outputId": "d57508c2-57e7-4b93-8184-0e90944d64f7"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Wyniki:\n",
            "      Roman  Actual Arabic  Predicted Arabic  Error\n",
            "0    CLXXII            172               172      0\n",
            "1   LXXXVII             87                87      0\n",
            "2       CLX            160               160      0\n",
            "3       LIX             59                60      1\n",
            "4        II              2                 2      0\n",
            "5      CXXV            125               125      0\n",
            "6    LXXIII             73                73      0\n",
            "7      XLIV             44                46      2\n",
            "8    CLXXIV            174               175      1\n",
            "9      CXIX            119               120      1\n",
            "10    CLVII            157               157      0\n",
            "11     CLIV            154               156      2\n",
            "12   CXCVII            197               197      0\n",
            "13  CLXVIII            168               169      1\n",
            "14       XX             20                20      0\n",
            "15     XXIV             24                25      1\n",
            "16      CIX            109               110      1\n",
            "17       XL             40                41      1\n",
            "18    CXLVI            146               145      1\n",
            "19     CLXI            161               161      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PODSUMOWANIE:\n",
        "- zadanie 1: model udało się wytrenować na danych, osiągając doskonałe wyniki z minimalnym błędem.\n",
        "- zadanie 2: konwersja liczb rzymskich na arabskie dawało bardzo dokładne przewidywania, gdzie model potrafił rozpoznać liczby rzymskie i dokładnie konwertować je na wartości arabskie. Błąd dla większości danych był bardzo niski (zaledwie kilka przypadków różnicy o 1 czy 2). Błąd dla większości testów wynosił 0 lub 1, co jest doskonałym wynikiem dla tego typu problemu.\n",
        "\n",
        "Zastosowanie LSTM w tym przypadku sprawdza się w doskonały sposób do przetwarzania sekwencji danych wejściowych. Dzięki poprawnemu preprocessingu (wektoryzacja liczb rzymskich, padding) oraz normalizacji wyników, model uczy się efektywnie, a predykcje są dokładne. To rozwiązanie stanowi przykład użycia RNN, LSTM i normalizacji danych w praktycznym problemie przetwarzania sekwencji. Dzięki tej metodzie, potrafimy skutecznie przewidywać konkretne liczby arabskie na podstawie znanych reguł liczb rzymskich."
      ],
      "metadata": {
        "id": "R2syOrTt_B-U"
      }
    }
  ]
}